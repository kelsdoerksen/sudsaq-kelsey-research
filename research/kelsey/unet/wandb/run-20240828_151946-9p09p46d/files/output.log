Grabbing training data...
Grabbing testing data...
Training lower bound model...
Training EPOCH 0:
> /Users/kelseydoerksen/code/sudsaq-kelsey-research/research/kelsey/unet/losses.py(102)forward()
    101         # Mask nans first
--> 102         mask = ~torch.isnan(y_true)
    103         # Add mask before calculating loss to remove nans
/Users/kelseydoerksen/opt/anaconda3/envs/aq/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
tensor([[[[0.1040, 0.1107, 0.1042,  ..., 0.1127, 0.1174, 0.1138],
          [0.1069, 0.1086, 0.1083,  ..., 0.1006, 0.1002, 0.1073],
          [0.1048, 0.1018, 0.1020,  ..., 0.1115, 0.1216, 0.1150],
          ...,
          [0.1077, 0.1104, 0.1087,  ..., 0.1080, 0.1040, 0.1041],
          [0.1103, 0.1078, 0.1078,  ..., 0.1029, 0.1020, 0.1063],
          [0.1074, 0.1013, 0.1042,  ..., 0.1017, 0.0996, 0.1054]]],
        [[[0.1026, 0.1170, 0.1049,  ..., 0.1165, 0.1158, 0.1178],
          [0.0950, 0.0878, 0.0931,  ..., 0.1006, 0.1104, 0.1187],
          [0.1027, 0.0985, 0.1010,  ..., 0.1139, 0.1186, 0.1031],
          ...,
          [0.1023, 0.1083, 0.1025,  ..., 0.1069, 0.1087, 0.1092],
          [0.1026, 0.1026, 0.1078,  ..., 0.1044, 0.1086, 0.1053],
          [0.1007, 0.1030, 0.1034,  ..., 0.1080, 0.1055, 0.0987]]],
        [[[0.1129, 0.1224, 0.1094,  ..., 0.1010, 0.1131, 0.1120],
          [0.1044, 0.1039, 0.1134,  ..., 0.1151, 0.1164, 0.1096],
          [0.0864, 0.1034, 0.1010,  ..., 0.1196, 0.0962, 0.1143],
          ...,
          [0.1023, 0.0978, 0.1035,  ..., 0.1088, 0.1086, 0.1040],
          [0.0988, 0.0978, 0.1034,  ..., 0.1057, 0.1084, 0.1008],
          [0.0996, 0.1037, 0.1040,  ..., 0.0997, 0.1040, 0.1050]]],
        ...,
        [[[0.1092, 0.1043, 0.0995,  ..., 0.1192, 0.1203, 0.1085],
          [0.1062, 0.1106, 0.0985,  ..., 0.1377, 0.1045, 0.1107],
          [0.0960, 0.0946, 0.0957,  ..., 0.1087, 0.1002, 0.1107],
          ...,
          [0.1014, 0.1018, 0.0986,  ..., 0.0977, 0.1122, 0.1189],
          [0.1019, 0.0997, 0.0988,  ..., 0.1081, 0.1071, 0.1043],
          [0.1002, 0.1044, 0.1071,  ..., 0.1017, 0.1029, 0.1010]]],
        [[[0.1048, 0.1111, 0.0960,  ..., 0.1039, 0.1079, 0.1031],
          [0.0998, 0.1065, 0.1051,  ..., 0.1024, 0.0975, 0.1122],
          [0.0922, 0.0853, 0.0950,  ..., 0.0964, 0.0997, 0.1196],
          ...,
          [0.1051, 0.0995, 0.1012,  ..., 0.0975, 0.1128, 0.1090],
          [0.1013, 0.1019, 0.0997,  ..., 0.1100, 0.0998, 0.1093],
          [0.1022, 0.0939, 0.1040,  ..., 0.1023, 0.1056, 0.1055]]],
        [[[0.1045, 0.1051, 0.1174,  ..., 0.0999, 0.1109, 0.1230],
          [0.0986, 0.1060, 0.0945,  ..., 0.1147, 0.1073, 0.1223],
          [0.1089, 0.1063, 0.1021,  ..., 0.1067, 0.0963, 0.1110],
          ...,
          [0.1010, 0.0962, 0.1000,  ..., 0.0946, 0.0987, 0.1065],
          [0.1041, 0.1022, 0.1086,  ..., 0.0988, 0.0981, 0.1050],
          [0.1021, 0.1055, 0.1037,  ..., 0.0996, 0.0990, 0.1046]]]],
       grad_fn=<ConvolutionBackward0>)
tensor([[[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          ...,
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan, 34.2279,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],
        [[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          ...,
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan, 38.5176,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],
        [[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          ...,
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],
        ...,
        [[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          ...,
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan, 38.2679,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],
        [[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          ...,
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan, 35.3602,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],
        [[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          ...,
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],
          [    nan,     nan,  7.8997,  ...,     nan,     nan,     nan],
          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]]])
Traceback (most recent call last):
  File "/Users/kelseydoerksen/code/sudsaq-kelsey-research/research/kelsey/unet/run_pipeline.py", line 123, in <module>
    run_cqr(unet, device, aq_train_dataset, aq_test_dataset, save_dir, experiment, 0.1, args.channels,
  File "/Users/kelseydoerksen/code/sudsaq-kelsey-research/research/kelsey/unet/run_cqr.py", line 235, in run_cqr
    lower_bound = cqr_training_loop(model, train_loader, lower_criterion, optimizer, grad_scaler, epochs, experiment,
  File "/Users/kelseydoerksen/code/sudsaq-kelsey-research/research/kelsey/unet/run_cqr.py", line 44, in cqr_training_loop
    loss = loss_criterion(outputs, labels)  # Calculate loss
  File "/Users/kelseydoerksen/opt/anaconda3/envs/aq/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/kelseydoerksen/code/sudsaq-kelsey-research/research/kelsey/unet/losses.py", line 102, in forward
    mask = ~torch.isnan(y_true)
  File "/Users/kelseydoerksen/code/sudsaq-kelsey-research/research/kelsey/unet/losses.py", line 102, in forward
    mask = ~torch.isnan(y_true)
  File "/Users/kelseydoerksen/opt/anaconda3/envs/aq/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/Users/kelseydoerksen/opt/anaconda3/envs/aq/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
If you suspect this is an IPython 8.7.0 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org
You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.
Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True